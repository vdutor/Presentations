---
title: "Score-based generative modelling: intro and ongoing research"
author: "Vincent Dutordoir"
institute: "University of Cambridge"
bibliography: references.bib
menu: false
slide-number: true
progress: false
format:
    revealjs:
        theme: [serif, custom.scss]
        smaller: false
        # Custom title slide
        template-partials:
            - title-slide.html
        # Config below is for step-by-step walk through code
        html-math-method: mathjax
        include-in-header:
        - text: |
            <script>
            MathJax = {
                options: {
                menuOptions: {
                    settings: {
                    assistiveMml: false
                    }
                }
                }
            };
            </script>
            <style>
            h1,h2,h3,h4,h5,p {color:black!important;}
            h1{font-size:2.5em!important}
            h2{font-size:1.25em!important}
            h3{font-size:1.0em!important; text-decoration: underline; font-weight: normal!important; }
            footnotesize{font-size:65%!important}
            scriptsize{font-size:50%!important}
            figcaption{font-style: italic; display: flex; flex-direction: row; justify-content: center; align-items:center;}
            .caption{font-style: italic; display: flex; flex-direction: row; justify-content: center; align-items:center;}
            .box {background-color:#eeeeee; padding: 4px 8px;}
            .box-highlight {background-color:#1f77b422; padding: 4px;}
            </style>
            <script src="https://cdn.tailwindcss.com"></script>
---
::: {style="font-size:3em;" data-auto-animate=true}
## Hi, I'm Vincent üëã
:::

::: {.hidden}
$$
\newcommand{\v}[1]{\boldsymbol{#1}} 
\newcommand{\m}[1]{\mathbf{#1}}
\newcommand{\c}[1]{\mathcal{#1}} 
\newcommand{\d}{\textrm{d}} 
\newcommand{\R}{\mathbb{R}} 
\newcommand{\E}{\mathbb{E}} 
$$
:::

::: {data-auto-animate=true}
## Hi, I'm Vincent üëã

::: {.incremental}
- Finished my Bachelor's and Masters at Ghent University in 2017
- Moved to Cambridge (UK) straight after to work for a startup PROWLER.io
- Started my PhD at Cambridge University in 2020
- Senior ML researcher at Secondmind.ai
:::

:::

## Outline

The talk will consist of two parts:

::: {.incremental}
1. A tutorially overview of energy-based models, and how this has led to diffusion models.
2. Diffusion models for stochastic processes.
:::

::: aside
Credits to Yang Song, Michael Hutchinson, and Arnaud Doucet for some of the images and slide material.
:::


## Generative modelling

::: {.incremental}
- Given: dataset $\{\v{x}_i\}_{i=1}^n$
- Goal:  fit a model $p_\theta(\v{x})$ to the data distribution 
:::

. . .

![Illustration generative modelling (from: openai.com)](figures/generative_modelling.svg)


## Energy-based models

A density defined through an *energy function* $U_\theta: \R^d \rightarrow \R$:
$$
p_{\theta}(\v{x}) = \frac{\textrm{e}^{-U_\theta(x)}}{Z_\theta},\quad 
$$

. . .

We can fit this energy function by maximising the log likelihood:
\begin{equation}
    \theta^* = \max_\theta \sum_{i=1}^N \log p_\theta(\v{x}_i)
\end{equation}

. . . 

Ô∏è‚ö†Ô∏èÔ∏è Intractable normalizing constant: $Z_\theta = \int_{\R^d} \textrm{e}^{-U_\theta(\v{x})}\d \v{x}$.


## Langevin dynamics
The density of $\v{x}_t$ as $t\rightarrow\infty$ for the SDE
$$
\d \v{x}_t = \nabla \log p(\v{x}_t) \d t + \sqrt{2} \d \v{W}_t
$$
is given by $p(\v{x})$, where $\v{W}_t$ is standard Brownian motion.

. . .

#### Euler-Marayuma
First-order discretization of continuous SDE. For a small stepsize $\gamma$ we can
$$
\v{x}_{k+1} = \v{x}_k + \gamma \nabla \log p(\v{x}_k) + \sqrt{2}\v{z}_k,\quad \v{z}_k \sim \c{N}(0,\gamma \m{I})
$$


---

![Using Langevin dynamics to sample from a mixture of two Gaussians. (from: Yang Song)](figures/langevin.gif)

## Score function

Modelling the density through the score function

$$
\nabla_{\v{x}} \log p(\v{x})
\approx s_\theta(\v{x}).
$$

. . .

The score of a density is independent of the normalizing constant.
Assume $p(x) = q(x) / Z$
$$
\nabla_x \log \frac{q(x)}{Z}  = \nabla_x \log q(x) - \nabla_x \log Z
$$





:::{.hidden}

## Learning the score (Explicit)

We would like to *explicitly* match a parametric score to the (true) score, minimising, with a loss like
\begin{equation}
    \ell_{\mathrm{esm}}({\mathbf{s}_\theta}) \triangleq \E_{p(\v{x})}\Big[{\|\nabla_{\v{x}} \log p(\v{x})\ - {\mathbf{s}_\theta}(\v{x})\|^2}\Big].
\end{equation}

. . .

‚ö†Ô∏è Requires having the true score of $p(\v{x})$...


## Learning the score (Implicit)

The explicit objective can be written *without* requiring the true score 

[@hyvarinen2005]

\begin{equation}
    \ell_{\mathrm{ism}}({\mathbf{s}_\theta}) \triangleq \E_{p(\v{x})}\Big[\nabla_{\v{x}} \cdot {\mathbf{s}_\theta}(\v{x}) + \frac{1}{2}\|{\mathbf{s}_\theta}(\v{x})\|^2\Big] = \frac{1}{2} \ell_{\mathrm{esm}}({\mathbf{s}_\theta})  + C
\end{equation}

:::

## Naive score-based generative modeling

![Score-based generative modeling with score matching + Langevin dynamics. (from: Yang Song)](figures/smld.jpeg)

## Pitfalls

![](figures/pitfalls.jpeg)

. . .

- Score is badly estimated in low-density areas

. . .

- Langevin dynamics has slow mixing rates


## Multiple noise levels

<!-- ![](figures/multinoise.jpeg) -->

@song2019generative suggest to perturb data points such that they populate low data density regimes.

::: {.fragment}
Consider a Markov chain
$\v{x}_0 \sim p_{0}$  and $\v{x}_{k+1} \sim p_{k+1|k}(\cdot | \v{x}_k)$, which gives
:::

::: {.fragment}
#### Forward
$$
p(\v{x}_{0:K}) = p_0(\v{x}_0) \prod_{k=0}^{K-1} p_{k+1|k}(\v{x}_{k+1} | \v{x}_k)
$$
:::

::: {.fragment}
#### Backward
$$
p(\v{x}_{0:K}) = p_K(\v{x}_K) \prod_{k=K-1}^{0} p_{k|k+1}(\v{x}_{k} | \v{x}_{k+1})
$$

. . .

where $p_{k|k+1}(\v{x}_{k} | \v{x}_{k+1})$ is unknown but can be obtained with Bayes' rule.
:::


## Generative modelling with multiple noise levels

![](figures/multinoise.jpeg)

Let
$$
p_0 = p_{data}
$$

Choose
$$
p_{k+1|k}(\v{x}_{k+1} | \v{x}_k) = \c{N}(\v{x}_{k+1}| \alpha \v{x}_k, (1 - \alpha^2)\m{I})
$$

such that for large enough $K$ we have
$$
p_K \approx p_{ref} = \c{N}(0, \m{I}).
$$


---

For sampling we need the *reverse* kernel $p_{k|k+1}$, given through Bayes' rule
$$
p_{k|k+1}(\v{x}_{k} | \v{x}_{k+1}) =  \frac{p_{k+1|k}(\v{x}_{k+1}|\v{x}_k)p_k(\v{x}_k)}{p_{k+1}(\v{x}_{k+1})}
$$
which is unfortunately intractable!

. . .

Using a Taylor approximation one can show
$$
p_{k|k+1}(\v{x}_{k} | \v{x}_{k+1}) \approx \c{N}\Big(\v{x}_k | (2 - \alpha) \v{x}_{k+1} + (1-\alpha^2) \nabla \log p_{k+1}(\v{x}_{k+1}), (1-\alpha^2)\m{I}\Big)
$$


- Approximate score with neural network

- Generate samples using backward process


## Score

Score is again *unavailable*, but using

$$
p_{k+1}(\v{x}_{k+1}) = \int p(\v{x}_0) p_{k+1|0}(\v{x}_{k+1} | \v{x}_0) \d \v{x}_{0}
$$

- Conditional expectation -> Regression problem

and Tweedie's formula [@efron2011tweedie] one can show that
$$
\nabla \log p_{k+1}(\v{x}_{k+1}) = \textrm{argmin}_{\theta} \Big[\mathbb{E}_{\v{x}_0,\v{x}_{k+1}} \| s_{\theta}(\v{x}_{k+1}) - \log p_{k+1|0}(\v{x}_{k+1}|\v{x}_0)\|^2\Big]
$$
<!-- = \mathbb{E}_{\v{x}|\v{x}_{k+1}}[\nabla \log p_{k+1|0}(\v{x}_{k+1}|\v{x}_k)] -->

## TODO: Example

## Perturbing data with an SDE

$$
\d \v{x}_t = f(\v{x}_t, t) \d t + g(t) \d \m{W}_t
$$
where $\m{W}_t$ is standard Brownian motion. Think of "$\d \m{W}/\d t \sim \c{N}(0, \d t)$".

![caption](figures/forward_sde.gif)


## Reverse

![caption](figures/reverse_sde.gif)


---

::: {.r-fit-text}
Part 2:

Diffusion Models for Stochastic Processes
:::

## Motivation

- Diffusion models have been used on different data modalities (molecules)

- Can we train a diffusion model to capture a distribution over functions?

## References

::: {#refs}
:::